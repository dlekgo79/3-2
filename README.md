# 인공지능 수학 4주차
--기계학습의 전반적인 과정--
1. regression과 classification의 차이
regression은 연속 변수를 예측하는 모델이고
classification은 클래스로 분류를 해주는 모델이다.

2. sample의 수를 표시 할 때 x^(i)이고 이때 i는 sample의 수이다.
따라서 첫번째 feature의 첫번째 원소는 표기시 x^(i)

3. 내가 만든 모델과 실제가 얼마나 가까운지 판단하게 해주는 함수가 cost function 함수이다.
MSE에서 예측 값 - 실제 값을 하여 오차를 구한다. 이 오차를 최소화 시키는 것이 최적의 parameter을 구하는 방식이고
그게 이 식의 목표다. 이 식에서 목적이 theta를 구하는 것이므로 변수 또한 theta이다. x와 y는 변수가 아니다.

4. Gradient Descent는 최적의 theta를 효율적으로 구하는 방법이다. 이의 원리는 random으로 점에서 기울기를 구한다.
이 기울기가 양수라면 우리는 가장 최소의 theta를 구하기 위해서 감소해야하므로 왼쪽으로 이동을 해야한다. 반대로 기울기가 음수라면 
오른쪽으로 이동을 해야한다. 이렇게 계속 반복을 하다보면 기울기가 0인 부분에 도착을 하게 될 것이다.

-----------------------------------------------------------------------------------------------------------------------
--선형대수학 개념--
벡터는 행렬의 특수 케이스이다.
Transpose는 vactor가 matrix여서 행렬의 특징을 받아서 가능한 것이다.
행렬에서 가장 중요한 것 중 하나는 내적이다. 내적은 표기를 x^(T)*y로 한다. 
